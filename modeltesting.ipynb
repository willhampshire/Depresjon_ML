{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| top_path: '/Users/williamhampshire/Desktop/pycharm/MLproject'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from icecream import ic # debugging output\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as DF  # create and typehint shorthand\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "sequence_length = 10 # X time series sequence length\n",
    "\n",
    "# interpretation of raw data for human analysis\n",
    "dataset_interpretation: dict = {\n",
    "    'gender': {\n",
    "        1: 'female',\n",
    "        2: 'male'\n",
    "    },\n",
    "    'afftype': {\n",
    "        1: 'bipolar II',\n",
    "        2: 'unipolar depressive',\n",
    "        3: 'bipolar I'\n",
    "    },\n",
    "    'melanch': {\n",
    "        1: 'melancholia',\n",
    "        2: 'no melancholia'\n",
    "    },\n",
    "    'inpatient': {\n",
    "        1: 'inpatient',\n",
    "        2: 'outpatient'\n",
    "    },\n",
    "    'marriage': {\n",
    "        1: 'married or cohabiting',\n",
    "        2: 'single'\n",
    "    },\n",
    "    'work': {\n",
    "        1: 'working or studying',\n",
    "        2: 'unemployed/sick leave/pension'\n",
    "    }\n",
    "}\n",
    "\n",
    "# convert data to ML compatible data\n",
    "dataset_interpretation_reversed: dict = {\n",
    "    'age': {\n",
    "        '20-24': 1.0,\n",
    "        '25-29': 2.0,\n",
    "        '30-34': 3.0,\n",
    "        '35-39': 4.0,\n",
    "        '40-44': 5.0,\n",
    "        '45-49': 6.0,\n",
    "        '50-54': 7.0,\n",
    "        '55-59': 8.0,\n",
    "        '60-64': 9.0,\n",
    "        '65-69': 10.0,\n",
    "    },\n",
    "    'gender': {\n",
    "        'female': 1.0,\n",
    "        'male': 2.0,\n",
    "    },\n",
    "    'work': {\n",
    "        'working or studying': 1.0,\n",
    "        'unemployed/sick leave/pension': 2.0\n",
    "    }\n",
    "}\n",
    "\n",
    "top_path = rf'{os.getcwd()}'  # raw strings allow consistent path slashes\n",
    "ic(top_path)\n",
    "\n",
    "data_path = top_path + r'/data' # if repo pulled fully\n",
    "\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epoch_losses = {'loss': [], 'val_loss': []}\n",
    "        #ic(self.epoch_losses)\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.epoch_losses['loss'].append(logs.get('loss'))\n",
    "        self.epoch_losses['val_loss'].append(logs.get('val_loss'))\n",
    "        #ic(self.epoch_losses)\n",
    "\n",
    "\n",
    "def interpret_values(row, conversions, float_conv=0) -> pd.Series:\n",
    "    for category in conversions:\n",
    "        if category in row:\n",
    "            if category == 'age':\n",
    "                row[category] = str(conversions[category].get(row[category], row[category]))\n",
    "            elif float_conv == 1:\n",
    "                row[category] = float(conversions[category].get(row[category], row[category]))\n",
    "            else:\n",
    "                row[category] = conversions[category].get(row[category], row[category])\n",
    "    return row\n",
    "\n",
    "condition_numbers: list = []\n",
    "\n",
    "def load_condition_data(scores_data_interpreted) -> dict:\n",
    "    numbers = scores_data_interpreted['number']\n",
    "    global condition_numbers  # call and edit the global variable\n",
    "    condition_numbers = [item for item in numbers if not item.startswith('control')]\n",
    "\n",
    "    condition: dict = {}\n",
    "    condition_path = data_path + r'/condition'\n",
    "\n",
    "    for num in condition_numbers:\n",
    "        condition_path_num = condition_path + rf'/{num}.csv'\n",
    "        activity_data_temp = pd.read_csv(condition_path_num)\n",
    "        new_activity_data_temp = DF()\n",
    "\n",
    "        new_activity_data_temp['timestamp'] = pd.to_datetime(activity_data_temp['timestamp'])\n",
    "        new_activity_data_temp['time_since_start[mins]'] = (new_activity_data_temp['timestamp'] - \\\n",
    "            new_activity_data_temp['timestamp'].iloc[0]).dt.total_seconds() / 60.0\n",
    "        new_activity_data_temp['activity'] = activity_data_temp['activity']\n",
    "\n",
    "        condition[num] = new_activity_data_temp\n",
    "\n",
    "    # ic(condition)\n",
    "    return condition # return explicitly despite global definition\n",
    "\n",
    "def load_scores() -> DF:\n",
    "    scores_path = data_path + r'/scores.csv'\n",
    "    scores_data = pd.read_csv(scores_path)\n",
    "    #ic(scores_data.head())  # check first few records in terminal output\n",
    "\n",
    "    scores_data_interpreted = DF()\n",
    "    for i, row in scores_data.iterrows():\n",
    "        row_interpreted = interpret_values(row, dataset_interpretation)\n",
    "        scores_data_interpreted = pd.concat([scores_data_interpreted, row_interpreted], axis=1)\n",
    "\n",
    "    scores_data_interpreted = scores_data_interpreted.T  # transpose\n",
    "\n",
    "    # ic(scores_data_interpreted.head())\n",
    "    return scores_data_interpreted\n",
    "\n",
    "def create_sequences(data, sequence_length) -> np.ndarray:\n",
    "    sequences = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequence = data[i:i + sequence_length]\n",
    "        sequences.append(sequence)\n",
    "    return np.array(sequences)\n",
    "\n",
    "def scale_and_prepare(scores: DF = None, condition: dict = None):\n",
    "    \"\"\"\n",
    "    Scales data and prepares data format for model training.\n",
    "    :param scores: DF\n",
    "    :param condition: dict\n",
    "    :return: patient_scaled_data dict, demographic_encoded DF, X_time_series np.ndarray\n",
    "    \"\"\"\n",
    "    # Scale the activity data\n",
    "    global sequence_length\n",
    "    scalers = {}\n",
    "    patient_scaled_data = {}\n",
    "    X_time_series = {}\n",
    "\n",
    "    for patient_id, patient_df in condition.items():\n",
    "        scaler = MinMaxScaler()\n",
    "        scaled_data = scaler.fit_transform(np.array(patient_df['activity']).reshape(-1, 1))\n",
    "        scalers[patient_id] = scaler\n",
    "        patient_scaled_data[patient_id] = scaled_data\n",
    "\n",
    "    for patient_id, scaled_data in patient_scaled_data.items():\n",
    "        X_time_series[patient_id] = create_sequences(scaled_data, sequence_length)\n",
    "\n",
    "    key_predictors = ['number', 'age', 'gender', 'madrs1', 'madrs2']\n",
    "\n",
    "    demographic_temp: DF = scores[scores['number'].str.startswith('condition')]\n",
    "\n",
    "    demographic = DF()\n",
    "    for i, field in enumerate(key_predictors):\n",
    "        demographic = pd.concat([demographic, demographic_temp[field]], axis=1)\n",
    "\n",
    "    # filter out all rows and columns except number condition_n, age, gender, madrs1, madrs2\n",
    "    demographic_encoded = DF()  # re-encode data back to integers\n",
    "    for i, row in demographic.iterrows():\n",
    "        row_interpreted = interpret_values(row, dataset_interpretation_reversed, float_conv=1)\n",
    "        # print(f\"{row=}, {row_interpreted=}\")\n",
    "        demographic_encoded = pd.concat([demographic_encoded, row_interpreted], axis=1)\n",
    "    demographic_encoded = demographic_encoded.T\n",
    "\n",
    "    #ic(demographic_encoded.head())\n",
    "\n",
    "    return patient_scaled_data, demographic_encoded, X_time_series\n",
    "\n",
    "def build_LSTM(time_series_shape, supplementary_shape) -> Model:\n",
    "    # Define LSTM branch for time series data with dropout and L2 regularization\n",
    "    time_series_input = Input(shape=time_series_shape, name='time_series_input')\n",
    "    x1 = LSTM(64, kernel_regularizer=l2(0.01))(time_series_input)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    # Define dense branch for supplementary data with dropout and L2 regularization\n",
    "    supplementary_input = Input(shape=supplementary_shape, name='supplementary_input')\n",
    "    x2 = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(supplementary_input)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "\n",
    "    # Merge branches\n",
    "    merged = concatenate([x1, x2])\n",
    "    output = Dense(1, activation='linear')(merged)\n",
    "\n",
    "    # Define and compile model\n",
    "    model = Model(inputs=[time_series_input, supplementary_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(), loss='mse')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def train_LSTM(scores: pd.DataFrame = None, condition: dict = None) -> Model:\n",
    "    patient_scaled_activity_dict, demographic_refined, X_time_series = scale_and_prepare(scores=scores, condition=condition)\n",
    "\n",
    "    X_time_series_combined = []\n",
    "    X_supplementary_combined = []\n",
    "    y_combined = []\n",
    "\n",
    "    for patient_id, sequences in X_time_series.items():\n",
    "        demographic_data = demographic_refined.loc[demographic_refined['number'] == patient_id].drop(columns=['number']).values\n",
    "        if len(demographic_data) == 0:\n",
    "            continue\n",
    "\n",
    "        demographic_data_repeated = np.repeat(demographic_data, sequences.shape[0], axis=0)\n",
    "        X_time_series_combined.append(sequences)\n",
    "        X_supplementary_combined.append(demographic_data_repeated)\n",
    "\n",
    "        y = demographic_refined.loc[demographic_refined['number'] == patient_id, 'madrs2'].values\n",
    "        if len(y) > 0:\n",
    "            y_combined.extend([y[0]] * sequences.shape[0])\n",
    "\n",
    "    X_time_series_combined = np.concatenate(X_time_series_combined, axis=0).astype('float32')\n",
    "    X_supplementary_combined = np.concatenate(X_supplementary_combined, axis=0).astype('float32')\n",
    "    y_combined = np.array(y_combined, dtype=np.float32)\n",
    "\n",
    "    model = build_LSTM(time_series_shape=(X_time_series_combined.shape[1], X_time_series_combined.shape[2]),\n",
    "                             supplementary_shape=(X_supplementary_combined.shape[1],))\n",
    "\n",
    "    # Split data into training and validation sets\n",
    "    X_train_ts, X_val_ts, X_train_sup, X_val_sup, y_train, y_val = train_test_split(X_time_series_combined,\n",
    "                                                                                    X_supplementary_combined,\n",
    "                                                                                    y_combined,\n",
    "                                                                                    test_size=0.3,\n",
    "                                                                                    random_state=42)\n",
    "\n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        # likely will not activate unless epoch number increased from 10\n",
    "    losses = LossHistory()\n",
    "    # Train the model\n",
    "    model_fitted = model.fit([X_train_ts, X_train_sup], y_train, epochs=30, batch_size=32,\n",
    "                             validation_data=([X_val_ts, X_val_sup], y_val), callbacks=[early_stopping, losses])\n",
    "    model.save('lstm_test.keras')\n",
    "    ic(losses.epoch_losses)\n",
    "    # Predict on validation data\n",
    "    y_pred = model.predict([X_val_ts, X_val_sup])\n",
    "\n",
    "    # Calculate accuracy metrics\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    print(f'Mean Squared Error: {mse:.3e}')\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(f'Root Mean Squared Error: {rmse:.3e}')\n",
    "\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    print(f'Mean Absolute Error: {mae:.3e}')\n",
    "\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "    print(f'R-squared: {r2:.3f}')\n",
    "\n",
    "    return model, losses\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T19:46:00.289290Z",
     "start_time": "2024-07-09T19:46:00.156946Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001B[1m12064/12064\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m95s\u001B[0m 8ms/step - loss: 10.5897 - val_loss: 0.1746\n",
      "Epoch 2/30\n",
      "\u001B[1m12064/12064\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m84s\u001B[0m 7ms/step - loss: 3.1527 - val_loss: 0.0992\n",
      "Epoch 3/30\n",
      "\u001B[1m12064/12064\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m149s\u001B[0m 8ms/step - loss: 2.1379 - val_loss: 0.0797\n",
      "Epoch 4/30\n",
      "\u001B[1m12064/12064\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m96s\u001B[0m 8ms/step - loss: 1.6010 - val_loss: 0.1168\n",
      "Epoch 5/30\n",
      "\u001B[1m12064/12064\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m88s\u001B[0m 7ms/step - loss: 1.3247 - val_loss: 0.2716\n",
      "Epoch 6/30\n",
      "\u001B[1m12064/12064\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m89s\u001B[0m 7ms/step - loss: 1.1960 - val_loss: 0.1482\n",
      "Epoch 7/30\n",
      "\u001B[1m 7307/12064\u001B[0m \u001B[32m━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━\u001B[0m \u001B[1m33s\u001B[0m 7ms/step - loss: 1.1636"
     ]
    }
   ],
   "source": [
    "scores_df = load_scores()  # dataframe of scores\n",
    "condition_dict_df = load_condition_data(scores_df)  # dict of key=condition_n, value=dataframe activity time series\n",
    "    # cols = timestamp, time_since_start[mins], activity\n",
    "models, losses = train_LSTM(scores=scores_df, condition=condition_dict_df)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-07-09T19:46:00.272606Z"
    }
   },
   "id": "e13185335f756ed3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "losses_df = DF(losses.epoch_losses)\n",
    "plt.figure(figsize=(6, 4))\n",
    "losses_df['epoch'] = losses_df.index + 1  # Add an epoch column for the x-axis\n",
    "\n",
    "sns.scatterplot(x='epoch', y='loss', data=losses_df, label='Training Loss', marker='x', s=100)\n",
    "sns.scatterplot(x='epoch', y='val_loss', data=losses_df, label='Validation Loss', marker='x', s=100)\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Model Losses (MSE)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.gca().xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "# Show the plot\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "c7c4f66e192a2241"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "20a5ddbd881e35c5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
