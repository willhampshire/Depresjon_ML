{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 21:20:35.026430: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "ic| top_path: '/Users/williamhampshire/Desktop/pycharm/MLproject'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from icecream import ic # debugging output\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as DF  # create and typehint shorthand\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "sequence_length = 10 # X time series sequence length\n",
    "\n",
    "# interpretation of raw data for human analysis\n",
    "dataset_interpretation: dict = {\n",
    "    'gender': {\n",
    "        1: 'female',\n",
    "        2: 'male'\n",
    "    },\n",
    "    'afftype': {\n",
    "        1: 'bipolar II',\n",
    "        2: 'unipolar depressive',\n",
    "        3: 'bipolar I'\n",
    "    },\n",
    "    'melanch': {\n",
    "        1: 'melancholia',\n",
    "        2: 'no melancholia'\n",
    "    },\n",
    "    'inpatient': {\n",
    "        1: 'inpatient',\n",
    "        2: 'outpatient'\n",
    "    },\n",
    "    'marriage': {\n",
    "        1: 'married or cohabiting',\n",
    "        2: 'single'\n",
    "    },\n",
    "    'work': {\n",
    "        1: 'working or studying',\n",
    "        2: 'unemployed/sick leave/pension'\n",
    "    }\n",
    "}\n",
    "\n",
    "# convert data to ML compatible data\n",
    "dataset_interpretation_reversed: dict = {\n",
    "    'age': {\n",
    "        '20-24': 1.0,\n",
    "        '25-29': 2.0,\n",
    "        '30-34': 3.0,\n",
    "        '35-39': 4.0,\n",
    "        '40-44': 5.0,\n",
    "        '45-49': 6.0,\n",
    "        '50-54': 7.0,\n",
    "        '55-59': 8.0,\n",
    "        '60-64': 9.0,\n",
    "        '65-69': 10.0,\n",
    "    },\n",
    "    'gender': {\n",
    "        'female': 1.0,\n",
    "        'male': 2.0,\n",
    "    },\n",
    "    'work': {\n",
    "        'working or studying': 1.0,\n",
    "        'unemployed/sick leave/pension': 2.0\n",
    "    }\n",
    "}\n",
    "\n",
    "top_path = rf'{os.getcwd()}'  # raw strings allow consistent path slashes\n",
    "ic(top_path)\n",
    "\n",
    "data_path = top_path + r'/data' # if repo pulled fully\n",
    "\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epoch_losses = {'loss': [], 'val_loss': []}\n",
    "        #ic(self.epoch_losses)\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.epoch_losses['loss'].append(logs.get('loss'))\n",
    "        self.epoch_losses['val_loss'].append(logs.get('val_loss'))\n",
    "        #ic(self.epoch_losses)\n",
    "\n",
    "\n",
    "def interpret_values(row, conversions, float_conv=0) -> pd.Series:\n",
    "    for category in conversions:\n",
    "        if category in row:\n",
    "            if category == 'age':\n",
    "                row[category] = str(conversions[category].get(row[category], row[category]))\n",
    "            elif float_conv == 1:\n",
    "                row[category] = float(conversions[category].get(row[category], row[category]))\n",
    "            else:\n",
    "                row[category] = conversions[category].get(row[category], row[category])\n",
    "    return row\n",
    "\n",
    "condition_numbers: list = []\n",
    "\n",
    "def load_condition_data(scores_data_interpreted) -> dict:\n",
    "    numbers = scores_data_interpreted['number']\n",
    "    global condition_numbers  # call and edit the global variable\n",
    "    condition_numbers = [item for item in numbers if not item.startswith('control')]\n",
    "\n",
    "    condition: dict = {}\n",
    "    condition_path = data_path + r'/condition'\n",
    "\n",
    "    for num in condition_numbers:\n",
    "        condition_path_num = condition_path + rf'/{num}.csv'\n",
    "        activity_data_temp = pd.read_csv(condition_path_num)\n",
    "        new_activity_data_temp = DF()\n",
    "\n",
    "        new_activity_data_temp['timestamp'] = pd.to_datetime(activity_data_temp['timestamp'])\n",
    "        new_activity_data_temp['time_since_start[mins]'] = (new_activity_data_temp['timestamp'] - \\\n",
    "            new_activity_data_temp['timestamp'].iloc[0]).dt.total_seconds() / 60.0\n",
    "        new_activity_data_temp['activity'] = activity_data_temp['activity']\n",
    "\n",
    "        condition[num] = new_activity_data_temp\n",
    "\n",
    "    # ic(condition)\n",
    "    return condition # return explicitly despite global definition\n",
    "\n",
    "def load_scores() -> DF:\n",
    "    scores_path = data_path + r'/scores.csv'\n",
    "    scores_data = pd.read_csv(scores_path)\n",
    "    #ic(scores_data.head())  # check first few records in terminal output\n",
    "\n",
    "    scores_data_interpreted = DF()\n",
    "    for i, row in scores_data.iterrows():\n",
    "        row_interpreted = interpret_values(row, dataset_interpretation)\n",
    "        scores_data_interpreted = pd.concat([scores_data_interpreted, row_interpreted], axis=1)\n",
    "\n",
    "    scores_data_interpreted = scores_data_interpreted.T  # transpose\n",
    "\n",
    "    # ic(scores_data_interpreted.head())\n",
    "    return scores_data_interpreted\n",
    "\n",
    "def create_sequences(data, sequence_length) -> np.ndarray:\n",
    "    sequences = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequence = data[i:i + sequence_length]\n",
    "        sequences.append(sequence)\n",
    "    return np.array(sequences)\n",
    "\n",
    "def scale_and_prepare(scores: DF = None, condition: dict = None):\n",
    "    \"\"\"\n",
    "    Scales data and prepares data format for model training.\n",
    "    :param scores: DF\n",
    "    :param condition: dict\n",
    "    :return: patient_scaled_data dict, demographic_encoded DF, X_time_series np.ndarray\n",
    "    \"\"\"\n",
    "    # Scale the activity data\n",
    "    global sequence_length\n",
    "    scalers = {}\n",
    "    patient_scaled_data = {}\n",
    "    X_time_series = {}\n",
    "\n",
    "    for patient_id, patient_df in condition.items():\n",
    "        scaler = MinMaxScaler()\n",
    "        scaled_data = scaler.fit_transform(np.array(patient_df['activity']).reshape(-1, 1))\n",
    "        scalers[patient_id] = scaler\n",
    "        patient_scaled_data[patient_id] = scaled_data\n",
    "\n",
    "    for patient_id, scaled_data in patient_scaled_data.items():\n",
    "        X_time_series[patient_id] = create_sequences(scaled_data, sequence_length)\n",
    "\n",
    "    key_predictors = ['number', 'age', 'gender', 'madrs1', 'madrs2']\n",
    "\n",
    "    demographic_temp: DF = scores[scores['number'].str.startswith('condition')]\n",
    "\n",
    "    demographic = DF()\n",
    "    for i, field in enumerate(key_predictors):\n",
    "        demographic = pd.concat([demographic, demographic_temp[field]], axis=1)\n",
    "\n",
    "    # filter out all rows and columns except number condition_n, age, gender, madrs1, madrs2\n",
    "    demographic_encoded = DF()  # re-encode data back to integers\n",
    "    for i, row in demographic.iterrows():\n",
    "        row_interpreted = interpret_values(row, dataset_interpretation_reversed, float_conv=1)\n",
    "        # print(f\"{row=}, {row_interpreted=}\")\n",
    "        demographic_encoded = pd.concat([demographic_encoded, row_interpreted], axis=1)\n",
    "    demographic_encoded = demographic_encoded.T\n",
    "\n",
    "    #ic(demographic_encoded.head())\n",
    "\n",
    "    return patient_scaled_data, demographic_encoded, X_time_series\n",
    "\n",
    "def build_LSTM(time_series_shape, supplementary_shape) -> Model:\n",
    "    # Define LSTM branch for time series data with dropout and L2 regularization\n",
    "    time_series_input = Input(shape=time_series_shape, name='time_series_input')\n",
    "    x1 = LSTM(64, kernel_regularizer=l2(0.01))(time_series_input)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    # Define dense branch for supplementary data with dropout and L2 regularization\n",
    "    supplementary_input = Input(shape=supplementary_shape, name='supplementary_input')\n",
    "    x2 = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(supplementary_input)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "\n",
    "    # Merge branches\n",
    "    merged = concatenate([x1, x2])\n",
    "    output = Dense(1, activation='linear')(merged)\n",
    "\n",
    "    # Define and compile model\n",
    "    model = Model(inputs=[time_series_input, supplementary_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(), loss='mse')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def train_LSTM(scores: pd.DataFrame = None, condition: dict = None) -> Model:\n",
    "    patient_scaled_activity_dict, demographic_refined, X_time_series = scale_and_prepare(scores=scores, condition=condition)\n",
    "\n",
    "    X_time_series_combined = []\n",
    "    X_supplementary_combined = []\n",
    "    y_combined = []\n",
    "\n",
    "    for patient_id, sequences in X_time_series.items():\n",
    "        demographic_data = demographic_refined.loc[demographic_refined['number'] == patient_id].drop(columns=['number']).values\n",
    "        if len(demographic_data) == 0:\n",
    "            continue\n",
    "\n",
    "        demographic_data_repeated = np.repeat(demographic_data, sequences.shape[0], axis=0)\n",
    "        X_time_series_combined.append(sequences)\n",
    "        X_supplementary_combined.append(demographic_data_repeated)\n",
    "\n",
    "        y = demographic_refined.loc[demographic_refined['number'] == patient_id, 'madrs2'].values\n",
    "        if len(y) > 0:\n",
    "            y_combined.extend([y[0]] * sequences.shape[0])\n",
    "\n",
    "    X_time_series_combined = np.concatenate(X_time_series_combined, axis=0).astype('float32')\n",
    "    X_supplementary_combined = np.concatenate(X_supplementary_combined, axis=0).astype('float32')\n",
    "    y_combined = np.array(y_combined, dtype=np.float32)\n",
    "\n",
    "    model = build_LSTM(time_series_shape=(X_time_series_combined.shape[1], X_time_series_combined.shape[2]),\n",
    "                             supplementary_shape=(X_supplementary_combined.shape[1],))\n",
    "\n",
    "    # Split data into training and validation sets\n",
    "    X_train_ts, X_val_ts, X_train_sup, X_val_sup, y_train, y_val = train_test_split(X_time_series_combined,\n",
    "                                                                                    X_supplementary_combined,\n",
    "                                                                                    y_combined,\n",
    "                                                                                    test_size=0.3,\n",
    "                                                                                    random_state=42)\n",
    "\n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        # likely will not activate unless epoch number increased from 10\n",
    "    losses = LossHistory()\n",
    "    # Train the model\n",
    "    model_fitted = model.fit([X_train_ts, X_train_sup], y_train, epochs=30, batch_size=32,\n",
    "                             validation_data=([X_val_ts, X_val_sup], y_val), callbacks=[early_stopping, losses])\n",
    "    model.save('lstm_test.keras')\n",
    "    ic(losses.epoch_losses)\n",
    "    # Predict on validation data\n",
    "    y_pred = model.predict([X_val_ts, X_val_sup])\n",
    "\n",
    "    # Calculate accuracy metrics\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    print(f'Mean Squared Error: {mse:.3e}')\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(f'Root Mean Squared Error: {rmse:.3e}')\n",
    "\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    print(f'Mean Absolute Error: {mae:.3e}')\n",
    "\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "    print(f'R-squared: {r2:.3f}')\n",
    "\n",
    "    return model, losses\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-28T20:20:40.545891Z",
     "start_time": "2024-07-28T20:20:28.855180Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 7\u001B[0m\n\u001B[1;32m      4\u001B[0m condition_dict_df \u001B[38;5;241m=\u001B[39m load_condition_data(scores_df)  \u001B[38;5;66;03m# dict of key=condition_n, value=dataframe activity time series\u001B[39;00m\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;66;03m# cols = timestamp, time_since_start[mins], activity\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m models, losses \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_LSTM\u001B[49m\u001B[43m(\u001B[49m\u001B[43mscores\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscores_df\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcondition\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcondition_dict_df\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_save_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmodel_testing.keras\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[1], line 246\u001B[0m, in \u001B[0;36mtrain_LSTM\u001B[0;34m(scores, condition, model_save_name)\u001B[0m\n\u001B[1;32m    245\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain_LSTM\u001B[39m(scores: pd\u001B[38;5;241m.\u001B[39mDataFrame \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, condition: \u001B[38;5;28mdict\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, model_save_name: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdefault_name\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Model:\n\u001B[0;32m--> 246\u001B[0m     patient_scaled_activity_dict, demographic_refined, X_time_series \u001B[38;5;241m=\u001B[39m \u001B[43mscale_and_prepare\u001B[49m\u001B[43m(\u001B[49m\u001B[43mscores\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscores\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcondition\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcondition\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    248\u001B[0m     X_time_series_combined \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    249\u001B[0m     X_supplementary_combined \u001B[38;5;241m=\u001B[39m []\n",
      "Cell \u001B[0;32mIn[1], line 184\u001B[0m, in \u001B[0;36mscale_and_prepare\u001B[0;34m(scores, condition)\u001B[0m\n\u001B[1;32m    181\u001B[0m         max_length \u001B[38;5;241m=\u001B[39m scaled_data_len\n\u001B[1;32m    183\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m patient_id, scaled_data \u001B[38;5;129;01min\u001B[39;00m patient_scaled_data\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m--> 184\u001B[0m     X_time_series[patient_id] \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_sequences\u001B[49m\u001B[43m(\u001B[49m\u001B[43mscaled_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msequence_length\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    187\u001B[0m key_predictors \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnumber\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mage\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgender\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmadrs1\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmadrs2\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m    188\u001B[0m add_predictor \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdeltamadrs\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
      "Cell \u001B[0;32mIn[1], line 155\u001B[0m, in \u001B[0;36mcreate_sequences\u001B[0;34m(data, sequence_length, max_super_sequence_size, mask)\u001B[0m\n\u001B[1;32m    152\u001B[0m finished_sequences \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mpad(sequences, shape, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mconstant\u001B[39m\u001B[38;5;124m'\u001B[39m, constant_values\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    154\u001B[0m \u001B[38;5;66;03m#ic(max_super_sequence_size, finished_sequences, finished_sequences.shape)\u001B[39;00m\n\u001B[0;32m--> 155\u001B[0m time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m100\u001B[39m)\n\u001B[1;32m    156\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m finished_sequences\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "scores_df = load_scores()  # dataframe of scores\n",
    "condition_dict_df = load_condition_data(scores_df)  # dict of key=condition_n, value=dataframe activity time series\n",
    "    # cols = timestamp, time_since_start[mins], activity\n",
    "models, losses = train_LSTM(scores=scores_df, condition=condition_dict_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-28T20:39:02.107069Z",
     "start_time": "2024-07-28T20:20:40.526047Z"
    }
   },
   "id": "e13185335f756ed3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "losses_df = DF(losses.epoch_losses)\n",
    "plt.figure(figsize=(6, 4))\n",
    "losses_df['epoch'] = losses_df.index + 1\n",
    "\n",
    "sns.scatterplot(x='epoch', y='loss', data=losses_df, label='Training Error', marker='x', s=100)\n",
    "sns.scatterplot(x='epoch', y='val_loss', data=losses_df, label='Validation Error', marker='x', s=100)\n",
    "\n",
    "plt.title('Model Losses (MSE)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.gca().xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-28T20:39:02.113366Z",
     "start_time": "2024-07-28T20:39:02.110201Z"
    }
   },
   "id": "c7c4f66e192a2241"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from pandas import DataFrame as DF  # Optional if not already imported\n",
    "\n",
    "# .png (rasterised), .svg (vector graphic) output?\n",
    "image_save_type = ['svg', 'png']  # 'png' or 'svg'\n",
    "\n",
    "# get default deep colours\n",
    "colours = sns.color_palette('deep')\n",
    "ic(len(colours))\n",
    "\n",
    "losses_df = DF(losses.epoch_losses)\n",
    "\n",
    "# plot data\n",
    "plt.figure(figsize=(6, 4))\n",
    "new_losses_df = DF()\n",
    "new_losses_df['epoch'] = losses_df.index + 1\n",
    "new_losses_df['loss'] = losses_df['loss']\n",
    "new_losses_df['val_loss'] = losses_df['val_loss']\n",
    "ic(new_losses_df)\n",
    "losses_df = new_losses_df\n",
    "\n",
    "ax1 = sns.scatterplot(x='epoch', y='loss', data=losses_df, label='Training Error', \n",
    "                      color=colours[0], marker='+', s=100, linewidth=1.5)\n",
    "ax2 = ax1.twinx()\n",
    "sns.scatterplot(x='epoch', y='val_loss', data=losses_df, label='Validation Error', \n",
    "                color=colours[3], marker='x', s=75, linewidth=1.5, \n",
    "                ax=ax2)\n",
    "# patience\n",
    "plt.axvspan(10, 15, color='gray', alpha=0.3)\n",
    "plt.text(12.5, 0.2, 'Patience\\n(early_stoppage)', horizontalalignment='center', verticalalignment='center', fontsize=10)\n",
    "\n",
    "# create 1 single legend\n",
    "ax1.legend_.remove()\n",
    "ax2.legend_.remove()\n",
    "handles1, labels1 = ax1.get_legend_handles_labels()\n",
    "handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "handles = handles1 + handles2\n",
    "labels = labels1 + labels2\n",
    "plt.legend(handles, labels, facecolor='white', loc='upper right' ,framealpha=0.6)\n",
    "\n",
    "# title and labels\n",
    "plt.title('Model Losses (MSE) vs. Epoch to Identify Overfitting')\n",
    "ax1.set_xlabel('Epoch number')\n",
    "ax1.set_ylabel('Loss (Training)')\n",
    "ax1.set_ylim(0)\n",
    "ax2.set_ylabel('Loss (Validation)')\n",
    "ax1.set_xlim(0)\n",
    "plt.gca().xaxis.set_major_locator(ticker.MaxNLocator(integer=True))  # integers only on x scale \n",
    "\n",
    "# Save figure\n",
    "if 'png' in image_save_type:\n",
    "    plt.savefig(\"model_losses.png\", dpi=150, transparent=False)\n",
    "if 'svg' in image_save_type:\n",
    "    plt.savefig(\"model_losses.svg\", format='svg', bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-28T20:39:02.114913Z"
    }
   },
   "id": "63b7272e0b586e6e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "losses_df.to_csv(\"model_losses.csv\", index=False)\n",
    "# loss = training loss, val_loss = validation loss\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-28T20:39:02.127731Z",
     "start_time": "2024-07-28T20:39:02.119472Z"
    }
   },
   "id": "20a5ddbd881e35c5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-28T20:39:02.122566Z"
    }
   },
   "id": "d3f2315615383f41"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-28T20:39:02.124816Z"
    }
   },
   "id": "4a78ec0e4d9a4366"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
